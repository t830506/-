環境：
  Python 3.6.3 jupyter notebook (numpy、pandas、sklearn)
 
特徵：
  將原始資料的時間新增天數相關變數，如：月份、日期、第幾天…等。

1.	計算每個檔案數量相關變數，如：\
-	檔案出現數量、檔案被幾個顧客使用、檔案被幾個商品使用、檔案被幾個顧客X商品使用\
-	檔案出現第一天、檔案出現最後天、出現天數、出現期間、出現頻率\
-	檔案每天出現數量(第一天至第六天)、檔案每天被幾個顧客使用(第一天至第六天)、檔案每天被幾個商品使用(第一天至第六天)、檔案每天被幾個顧客X商品使用(第一天至第六天)

2.	檔案與小時相關變數，如：\
-	出現小時平均、出現小時標準差、出現過幾個時段、小時的分佈(ex: 0-5、6-11…)\
-	檔案每小時最大流量、檔案每小時幾個顧客X商品對的最大流量\
-	檔案第一天每小時最大流量平均、檔案第一天每小時最大流量標準差、檔案第一天每小時最大流量最大值、檔案第一天每小時最大流量最小值\
-	檔案第一天每小時幾個顧客X商品對的最大流量平均、檔案第一天每小時幾個顧客X商品對的最大流量標準差、檔案第一天每小時幾個顧客X商品對的最大流量最大值、檔案第一天每小時幾個顧客X商品對的最大流量最小值\
-	檔案每天每小時數量平均的平均、檔案每天每小時數量平均的極大減極小、檔案每天每小時數量標準差的平均、檔案每天每小時數量標準差的極大減極小、檔案每天每小時數量平均的第二天減第一天、檔案每天每小時數量標準差的第二天減第一天\
-	檔案每天每小時幾個顧客X商品對的平均的平均、、檔案每天每小時幾個顧客X商品對的平均的極大減極小、檔案每天每小時幾個顧客X商品對的標準差的平均、檔案每天每小時幾個顧客X商品對的標準差的極大減極小、檔案每天每小時幾個顧客X商品對的平均的第二天減第一天、檔案每天每小時幾個顧客X商品對的標準差的第二天減第一天

3.	檔案與秒數相關變數，如：\
-	檔案秒數分佈(ex: 0-14、15-29…)\
-	檔案出現間隔時間(s)的平均、檔案出現間隔時間(s)的標準差\
-	檔案出現間隔時間(s)等於零的比例、檔案出現間隔時間(s)在零到十的比例、檔案出現間隔時間(s)在十到三十的比例、檔案出現間隔時間(s)在三十到六十的比例、檔案出現間隔時間(s)在六十到三百的比例、檔案出現間隔時間(s)超過三百的比例\
-	檔案出現間隔時間(s)的平均(第一天到第三天)、檔案出現間隔時間(s)的標準差(第一天到第三天)\
-	檔案出現間隔時間(s)等於零的比例(第一天到第三天)、檔案出現間隔時間(s)在零到十的比例(第一天到第三天)、檔案出現間隔時間(s)在十到三十的比例(第一天到第三天)、檔案出現間隔時間(s)在三十到六十的比例(第一天到第三天)、檔案出現間隔時間(s)在六十到三百的比例(第一天到第三天)、檔案出現間隔時間(s)超過三百的比例(第一天到第三天)。

4.	相同概念也有計算一分鐘出現很多次的檔案、一小時出現很多次的檔案。

5.	mean encoding\
-	利用noise、數量當作smoothing term。\
-	對顧客ID做mean encoding，搭配每日更新的方式，例如：假設要算顧客第三天的分數，就需要利用前兩天的資料做mean encoding，才能得到顧客第三天的分數。合理推論第四天顧客分數為顧客前三天資料所算出。\
-	對商品ID做mean encoding，搭配每日更新的方式，例如：假設要算商品第三天的分數，就需要利用前兩天的資料做mean encoding，才能得到商品第三天的分數。合理推論第四天商品分數為商品前三天資料所算出。\
-	對顧客X商品ID做mean encoding，搭配每日更新的方式，例如：假設要算顧客X商品第三天的分數，就需要利用前兩天的資料做mean encoding，才能得到顧客X商品第三天的分數。合理推論第四天顧客X商品分數為顧客X商品前三天資料所算出。

6.	expanding mean encoding\
-	先計算檔案顧客唯一組合，再利用相同概念，先sort_value(日期)，當要計算顧客的分數時，只利用顧客以前的資料做計算，所以顧客分數在不同天時會有不同分數。\
-	以此類推，顧客X商品、顧客X商品X日的組合也能用此想法做mean encoding。

訓練模型：
GBM：\
透過gridsearch調整參數，利用random state與learning rate的排列組合，得到約18組模型與答案，ensemble後得到最終答案。

訓練方式與原始碼：\
  如同訓練模型中提到的，因為比賽過程中持續想到新的特徵，所以會一直不斷更新。再將每天得到的新答案與舊答案畫散佈圖，再做ensemble。

結論：\
  這次競賽中花費許多心力在特徵萃取，尤其在類別變數的轉換，如何在保持每個顧客的特性之餘，又不能有leak，所以如何做smoothing去防止過度擬合成為很重要的議題。此外，這次比賽最大的挑戰在於每天只能上傳兩次，所以有些特徵變數或結果應該利用視覺化或validation set去判定其效果，不該花太多上傳機會去嘗試，以利在後面做模型整合時有更多機會去嘗試。
